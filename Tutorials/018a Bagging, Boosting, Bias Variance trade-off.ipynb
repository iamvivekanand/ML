{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging, Boostin, Bias Variane tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forest is a bagging( an ensemble) techniques, because model are built in parallel\n",
    "- It builds decision trees on different samples and takes their majority vote for classification and average in case of regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ensemble Learning/Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble simply means combining multiple models.\n",
    "Thus a collection of models is used to make predictions rather than an individual model.\n",
    "\n",
    "Ensemble uses two types of methods:\n",
    "\n",
    "        1.Bagging\n",
    "        2.Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- also known as Bootstrap Aggregation\n",
    "- it chooses a random sample, each model is generated from this samples (Bootstrap Samples) provided by the Original Data with replacement, known as row sampling. \n",
    "- This step of row sampling with replacement is called bootstrap\n",
    "-  some observations may be repeated in each new training data set\n",
    "- so,In Bagging, every element has the same probability to appear in a new dataset\n",
    "- Now each model is trained independently which generates results. \n",
    "- The final output is based on majority voting after combining(Aggregation) the results of all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Multiple subsets are created from the original data set with equal tuples, selecting observations with replacement.\n",
    "2. A base model is created on each of these subsets.\n",
    "3. Each model is learned in parallel with each training set and independent of each other.\n",
    "4. The final predictions are determined by combining the predictions from all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Firstly, a model is built from the training data. \n",
    "    Then the second model is built which tries to correct the errors present in the first model. \n",
    "    This procedure is continued and models are added until either the complete training data set \n",
    "      is predicted correctly or the maximum number of models is added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialise the dataset and assign equal weight to each of the data point.\n",
    "2. Provide this as input to the model and identify the wrongly classified data points.\n",
    "3. Increase the weight of the wrongly classified data points and decrease the weights of correctly classified data points. \n",
    "4. And then normalize the weights of all data points.\n",
    "5. if (got required results) model is built or ends here, else Goto step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias and Variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of prediction error: \n",
    "- bias, \n",
    "- variance, and \n",
    "- irreducible error. \n",
    "\n",
    "Irreducible error, also known as “noise,” can’t be reduced by the choice of algorithm. \n",
    "\n",
    "The other two types of errors, however, can be reduced because they stem from your algorithm choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is an assumption made by a model to make the target function easier to learn. Models with high bias are less flexible and are not fully able to learn from the training data.\n",
    "\n",
    "Such as a linear regression model, the regression line fails to fit the majority of the data points and thus, this model has high bias and low learning power. Generally, models with low bias are preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it defines how much the predictions of a model will change from one dataset to another\n",
    "- It can also be defined as the Difference between actual and predicted data\n",
    "- Ideally, we want a model with low variance, model with low variance would mean that the difference between actual and predicted value is low\n",
    "- But there seem to be tradeoffs between the bias and variance. This is known as a bias-variance tradeoff. Hence when we decrease one, the other increases, and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble methods can be divided into two groups:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel ensemble methods:\n",
    "- or Bagging techniques\n",
    "- base learners are generated in parallel simultaneously\n",
    "\n",
    "Sequential ensemble methods:\n",
    "- or Boosting techniques\n",
    "- different learners learn sequentially with early learners fitting simple models to the data. Then the data is analyzed for errors. The goal is to solve for net error from the prior mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homogeneous & heterogeneous ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most ensemble methods use a single base learning algorithm to produce homogeneous base learners i.e. learners of the same type, leading to homogeneous ensembles.\n",
    "\n",
    "For example, Random forests & Adaboost both do homogeneous ensembling\n",
    "\n",
    "Some methods use learners of different types as base learners.\n",
    "In Scikit-learn, there is a model known as a voting classifier. This is an example of heterogeneous learners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Steps involved in random forest algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. 'n' number of random features are taken from the data set having 'k' number of records.\n",
    "2. Individual decision trees are constructed for each sample.\n",
    "3. Each decision tree will generate an output.\n",
    "4. Final output is considered based on Majority Voting or Averaging for Classification and regression respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important Features of Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Diversity:\n",
    "Not all attributes/variables/features are considered while making an individual tree, each tree is different.\n",
    "\n",
    "2. Immune to the curse of dimensionality- Since each tree does not consider all the features, the feature space is reduced.\n",
    "\n",
    "3. Parallelization:\n",
    "Each tree is created independently out of different data and attributes. This means that we can make full use of the CPU to build random forests.\n",
    "\n",
    "4.  Train-Test split:In a random forest we don’t have to segregate the data for train and test as there will always be 30% of the data which is not seen by the decision tree.\n",
    "\n",
    "5.  Stability- Stability arises because the result is based on majority voting/ averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Difference Between Decision Tree & Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees normally suffer from the problem of overfitting if it’s allowed to grow without any control.\n",
    "- While in RandomForest, final output is based on average or majority ranking and hence the problem of overfitting is taken care of..\n",
    "- A single decision tree is faster in computation.\n",
    "- Random Forest is comparatively slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important Hyperparameters Of Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following hyperparameters increases the predictive power:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. n_estimators: number of trees the algorithm builds before averaging the predictions.\n",
    "2. max_features: maximum number of features random forest considers while splitting a node.\n",
    "3. mini_sample_leaf: determines the minimum number of leaves required to split an internal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following hyperparameters increases the speed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. n_jobs– it tells the engine how many processors it is allowed to use. If the value is 1, it can use only one processor but if the value is -1 there is no limit.\n",
    "\n",
    "2. random_state– controls randomness of the sample. The model will always produce the same results if it has a definite value of random state and if it has been given the same hyperparameters and the same training data.\n",
    "\n",
    "3. oob_score – OOB means out of the bag. It is a random forest cross-validation method. In this one-third of the sample is not used to train the data instead used to evaluate its performance. These samples are called out of bag samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementaiton of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>BP</th>\n",
       "      <th>cholestrol</th>\n",
       "      <th>heart disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>322</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex   BP  cholestrol  heart disease\n",
       "0   70    1  130         322              1\n",
       "1   67    0  115         564              0\n",
       "2   57    1  124         261              1\n",
       "3   64    1  128         263              0\n",
       "4   74    0  120         269              0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('heart_v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('heart disease',axis=1)\n",
    "# Putting response variable to y\n",
    "y = df['heart disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((189, 4), (81, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,n_jobs=-1,max_depth=5,random_state=42,oob_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    max_depth : int, default=None\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 372 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, n_jobs=-1, oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.656084656084656"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s do hyperparameter tuning for Random Forest using GridSearchCV and fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another instance of the classifier\n",
    "rfc = RandomForestClassifier(random_state=42,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to store parameters listings\n",
    "params={\n",
    "    'max_depth':[2,3,5,10,20],\n",
    "    'min_samples_leaf':[5,10,20,50,100,200],\n",
    "    'n_estimators':[10,25,30,50,100,200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GridsearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an instance of GridsearchCV\n",
    "gridcv=GridSearchCV(estimator=rfc,param_grid=params,cv=4,n_jobs=-1,verbose=1,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 180 candidates, totalling 720 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, estimator=RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [2, 3, 5, 10, 20],\n",
       "                         'min_samples_leaf': [5, 10, 20, 50, 100, 200],\n",
       "                         'n_estimators': [10, 25, 30, 50, 100, 200]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the data on GridsearchCV\n",
    "gridcv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6985815602836879"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, min_samples_leaf=10, n_estimators=10,\n",
       "                       n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best = gridcv.best_estimator_\n",
    "rf_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The above output gives the best parameters values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
