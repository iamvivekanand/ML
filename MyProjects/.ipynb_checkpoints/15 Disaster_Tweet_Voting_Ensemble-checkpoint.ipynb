{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaster_Tweet_Voting_Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In simple words stemming is reducing a word to its base word or stem in such a way that the words of similar kind lie under a common stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stemming is the process of producing morphological variants of a root/base word.\n",
    "e.g. stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Errors in Stemming: \n",
    "There are mainly two errors in stemming – Overstemming and Understemming. Overstemming occurs when two words are stemmed to same root that are of different stems. Under-stemming occurs when two words are stemmed to same root that are not of different stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"disaster-tweets-train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"disaster-tweets-test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 5), (3263, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.27203467752528"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df['location'].isnull().sum()/len(train_df['location']))*100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can approx 33 percent values are missing from location column, we can't drop it directly otherwise we will loose import information.\n",
    "let explore this column more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Birmingham', 'Est. September 2012 - Bristol', ...,\n",
       "       'Vancouver, Canada', 'London ', 'Lincoln'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop([\"location\"], axis=1, inplace=True)\n",
    "test_df.drop([\"location\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['keyword']=train_df['keyword'].fillna(train_df['keyword'].value_counts().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['keyword'] = test_df['keyword'].fillna(test_df['keyword'].value_counts().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "keyword    0\n",
      "text       0\n",
      "target     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "keyword    0\n",
      "text       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     keyword                                               text  target\n",
       "0   1  fatalities  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1   4  fatalities             Forest fire near La Ronge Sask. Canada       1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing id columns from both training and testing dataset, as they have no means to us\n",
    "train_df.drop([\"id\"], axis=1, inplace=True)\n",
    "test_df.drop([\"id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra punctuations\n",
    "train_df[\"text\"].replace(\"[^a-zA-Z]\", \" \",regex = True, inplace = True)\n",
    "test_df[\"text\"].replace(\"[^a-zA-Z]\", \" \",regex = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all letter to lower case\n",
    "train_df['text']=train_df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text']=test_df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-862efa64e45d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# removing stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not callable"
     ]
    }
   ],
   "source": [
    "# removing stopwords\n",
    "train_df['text']=train_df['text'].apply(lambda x: x for x in train_df['text'] if x not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [j, u,  , h, p, p, e, n, e,  ,  , e, r, r, b, ...\n",
       "1       [h, e, r,  , b, u,  ,  , e, r, h, q, u, k, e, ...\n",
       "2       [h, e, r, e,  ,  ,  , f, r, e,  , f, r, e,  , ...\n",
       "3       [p, c, l, p, e,  , l, g, h, n, g,  ,  ,  , p, ...\n",
       "4       [p, h, n,  , u, e, l, r,  , k, l, l,  ,  ,  , ...\n",
       "                              ...                        \n",
       "3258    [e, r, h, q, u, k, e,  , f, e,  , l,  , n, g, ...\n",
       "3259    [r,  , n,  , r,  , w, r, e,  , h, n,  , l,  , ...\n",
       "3260    [g, r, e, e, n,  , l, n, e,  , e, r, l, e, n, ...\n",
       "3261    [e, g,  , u, e,  , h, z, r, u,  , w, e, h, e, ...\n",
       "3262    [ , c, f, c, l, g, r,  , h,  , c, v, e,  ,  , ...\n",
       "Name: text, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['text'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (8.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(train_df[train_df.target == 1].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fatalities</td>\n",
       "      <td>[u, r,  , e, e,  , r, e,  , h, e,  , r, e, n, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fatalities</td>\n",
       "      <td>[f, r, e,  , f, r, e,  , n, e, r,  , l,  , r, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword                                               text  target\n",
       "0  fatalities  [u, r,  , e, e,  , r, e,  , h, e,  , r, e, n, ...       1\n",
       "1  fatalities  [f, r, e,  , f, r, e,  , n, e, r,  , l,  , r, ...       1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [u, r,  , e, e,  , r, e,  , h, e,  , r, e, n, ...\n",
       "1    [f, r, e,  , f, r, e,  , n, e, r,  , l,  , r, ...\n",
       "2    [l, l,  , r, e, e, n,  , k, e,  ,  ,  , h, e, ...\n",
       "3    [ ,  ,  ,  ,  ,  ,  , p, e, p, l, e,  , r, e, ...\n",
       "4    [j, u,  , g,  , e, n,  , h,  , p, h,  , f, r, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['target'],axis = 1)\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X[\"sentence\"] = X['keyword'] + \" \" + X['text']\n",
    "X_train = np.array(X[\"sentence\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_df[\"sentence\"] = test_df['keyword'] + \" \" + test_df['text']\n",
    "Xtest = np.array(test_df[\"sentence\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,  TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(Xtrain)\n",
    "\n",
    "keyword = vectorizer.get_feature_names()\n",
    "x_train = vectorizer.transform(Xtrain)\n",
    "x_test = vectorizer.transform(Xtest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "model2 = ComplementNB()\n",
    "model3 = SVC()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "final_model = VotingClassifier(estimators=[('lOG', model1), ('NB', model2), ('SVC',model3)], voting='hard')\n",
    "\n",
    "final_model.fit(x_train, Y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred = final_model.predict(x_test)\n",
    "\n",
    "## filling submission.csv\n",
    "\n",
    "submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "\n",
    "submission[\"target\"] = pred\n",
    "submission.to_csv(\"voting_ensemble.csv\", index=False)\n",
    "\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
