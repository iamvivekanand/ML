{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- its a SML techniques, its a type of classification algorithm\n",
    "- but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n",
    "- Instead of fitting a regression line, we fit an \"S\" shaped logistic function, which predicts two maximum values (0 or 1).\n",
    "- This S shaped curve indicate Sigmoid function or logistic function\n",
    "- maps any real value within a range of 0 and 1.\n",
    "- In logistic regression, we use the concept of the threshold value,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='logistic-equation.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        Image source: Javatpoint.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How may types of Logistic Regression are there??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Logistic Regression can be classified into three types:\n",
    "    Binomial LR: \n",
    "    only two possible types of output, e.g. 0 or 1, Pass or Fail, etc.\n",
    "\n",
    "    Multinomial LR:\n",
    "    3 or more possible ouputs, but there r not ordered \n",
    "    such as \"cat\", \"dogs\", or \"sheep\"\n",
    "\n",
    "    Ordinal LR: \n",
    "    3 or more possible output, but they are ordered. Such as low,Medium,High, OR good, better and best etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=sns.load_dataset('iris')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,0:4].values\n",
    "y=df.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species=le.fit_transform(df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesuring accuracy of the model\n",
    "\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, \n",
      "          one of [None, 'micro', 'macro', 'weighted']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    recall_score(y_test,y_pred)\n",
    "except:\n",
    "    print('''ValueError: Target is multiclass but average='binary'. Please choose another average setting, \n",
    "          one of [None, 'micro', 'macro', 'weighted']''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9672820512820512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Parameters of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr=LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (penalty='l2',\n",
    "    dual=False,\n",
    "    tol=0.0001,\n",
    "    C=1.0,\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1,\n",
    "    class_weight=None,\n",
    "    random_state=None,\n",
    "    solver='lbfgs',\n",
    "    max_iter=100,\n",
    "    multi_class='auto',\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    n_jobs=None,\n",
    "    l1_ratio=None)\n",
    "\n",
    "    Docstring:     \n",
    "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "    This class implements regularized logistic regression using the\n",
    "    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "    that regularization is applied by default**. It can handle both dense\n",
    "    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "    floats for optimal performance; any other input format will be converted\n",
    "    (and copied).\n",
    "\n",
    "    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "    with primal formulation, or no regularization. The 'liblinear' solver\n",
    "    supports both L1 and L2 regularization, with a dual formulation only for\n",
    "    the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "    'saga' solver.\n",
    "\n",
    "    Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "    penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
    "        Used to specify the norm used in the penalization. The 'newton-cg',\n",
    "        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
    "        only supported by the 'saga' solver. If 'none' (not supported by the\n",
    "        liblinear solver), no regularization is applied.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    dual : bool, default=False\n",
    "        Dual or primal formulation. Dual formulation is only implemented for\n",
    "        l2 penalty with liblinear solver. Prefer dual=False when\n",
    "        n_samples > n_features.\n",
    "\n",
    "    tol : float, default=1e-4\n",
    "        Tolerance for stopping criteria.\n",
    "\n",
    "    C : float, default=1.0\n",
    "        Inverse of regularization strength; must be a positive float.\n",
    "        Like in support vector machines, smaller values specify stronger\n",
    "        regularization.\n",
    "\n",
    "    fit_intercept : bool, default=True\n",
    "        Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "        added to the decision function.\n",
    "\n",
    "    intercept_scaling : float, default=1\n",
    "        Useful only when the solver 'liblinear' is used\n",
    "        and self.fit_intercept is set to True. In this case, x becomes\n",
    "        [x, self.intercept_scaling],\n",
    "        i.e. a \"synthetic\" feature with constant value equal to\n",
    "        intercept_scaling is appended to the instance vector.\n",
    "        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "    class_weight : dict or 'balanced', default=None\n",
    "        Weights associated with classes in the form ``{class_label: weight}``.\n",
    "        If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "    random_state : int, RandomState instance, default=None\n",
    "        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "        data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
    "    - 'liblinear' and 'saga' also handle L1 penalty\n",
    "    - 'saga' also supports 'elasticnet' penalty\n",
    "    - 'liblinear' does not support setting ``penalty='none'``\n",
    "\n",
    "    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
    "    features with approximately the same scale. You can\n",
    "    preprocess the data with a scaler from sklearn.preprocessing.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "        If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "        label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "        across the entire probability distribution, *even when the data is\n",
    "        binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "        and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "    verbose : int, default=0\n",
    "        For the liblinear and lbfgs solvers set verbose to any positive\n",
    "        number for verbosity.\n",
    "\n",
    "    warm_start : bool, default=False\n",
    "        When set to True, reuse the solution of the previous call to fit as\n",
    "        initialization, otherwise, just erase the previous solution.\n",
    "        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "    n_jobs : int, default=None\n",
    "        Number of CPU cores used when parallelizing over classes if\n",
    "        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "        set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "        context. ``-1`` means using all processors.\n",
    "        See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "    l1_ratio : float, default=None\n",
    "        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "        combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "    classes_ : ndarray of shape (n_classes, )\n",
    "        A list of class labels known to the classifier.\n",
    "\n",
    "    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "        Coefficient of the features in the decision function.\n",
    "\n",
    "        `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "    intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "        Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "        If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "        `intercept_` is of shape (1,) when the given problem is binary.\n",
    "        In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "        outcome 0 (False).\n",
    "\n",
    "    n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "        Actual number of iterations for all classes. If binary or multinomial,\n",
    "        it returns only 1 element. For liblinear solver, only the maximum\n",
    "        number of iteration across all classes is given.\n",
    "\n",
    "        .. versionchanged:: 0.20\n",
    "\n",
    "            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
