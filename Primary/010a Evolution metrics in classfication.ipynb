{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution metrics in Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-recall is a widely used metrics for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are various evaluation metrics used for classification algorithm:\n",
    "\n",
    "    confusion matrix\n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    precision_recall_curve,                     \n",
    "    precision_recall_fscore_support, \n",
    "    auc,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy measures the ratio of:\n",
    "\n",
    "    Number of correct predictions/total number of predictions\n",
    "    \n",
    "    accuracy=TP+TN/TP+TN+FP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one metric won’t tell the entire story.\n",
    "- Like in case of accuracy if output has a total of 100 sample and out of these 99 are of positive class, then our model will always predict positive class except one. in this way the model will get 99% of accuracy.\n",
    "- Accuracy is useful when the target class is well balanced but is not a good choice for the unbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- also called error matrix\n",
    "- a matrix used to determine the performance of the classification models,where the output can help more than 2 class\n",
    "- it is a combination of Actual value and predictive value\n",
    "\n",
    "        Actual values are the available values from the test data.\n",
    "        Predicted values are from model's prediction\n",
    "- Confusion matrix consist of following variables TP,TN,FP,FN\n",
    "\n",
    "        True Negative: predicted No, actual No.\n",
    "\n",
    "        True Positive: predicted yes, actual yes\n",
    "\n",
    "        False Positive: Predicted Yes, Actual No. \n",
    "        ((Type-I error))\n",
    "\n",
    "        False Negative: predicted no, actual Yes, \n",
    "        ((Type-II error))\n",
    "\n",
    "- For 2 prediction classes, matrix is a 2*2 table, for 3 classes, it is 3*3 table, and so on\n",
    "     \n",
    "         False Positive (Type 1 Error)\n",
    "         False Negative (Type 2 Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What is the use/need of Confusion Matrix?? Describe the role of Confusion matrix in determining Model performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It evaluates the performance of the classification models by comparing Predicted values and Actual values of Test Data\n",
    "We can calculate the different parameters for the model, such as accuracy, precision, etc. using Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### precision_score\n",
    "- precision is \"number of true positives divided by the number of predicted positives\"\n",
    "- useful in the cases where False Positive is more important than False Negatives\n",
    "\n",
    "        TP/(TP+FP)\n",
    "- To keep the precision high, FP should be as low as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recall_score:\n",
    "- Recall or sensitivity \n",
    "- useful in cases where False Negative is of higher concern than False Positive.\n",
    "- like in medical feild if a patient had disease it shouldn't be predicted as not having disease.\n",
    "\n",
    "        Recall= TP / (TP+FN)\n",
    "- To keep the recall high, FN should be as low as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1_score\n",
    "- it is the harmonic mean of precision and recall\n",
    "- It is maximum when Precision is equal to Recall\n",
    "- F1 Score could be used more, Where\n",
    "\n",
    "        When FP and FN are equally costly.\n",
    "        Adding more data doesn’t effectively change the outcome\n",
    "        True Negative is high\n",
    "        \n",
    "     **f1_score= 2 * ((precison * recall) / (precision + recall))**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### auc,roc,roc_auc_score\n",
    "- Receiver Operator Characteristic (ROC) is a probability curve that plots the TPR(True Positive Rate) against the FPR(False Positive Rate) at various threshold values and separates the ‘signal’ from the ‘noise’.\n",
    "-  Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes. \n",
    "-  greater the AUC, the better is the performance of the model at different threshold points between positive and negative classes\n",
    "\n",
    "        >When AUC is equal to 1, the classifier perfectly distinguish between all Positive and Negative class points. \n",
    "        >When AUC is equal to 0, the classifier would be predicting all Negatives as Positives and vice versa. \n",
    "        >When AUC is 0.5, the classifier doesn't distinguish between the Positive and Negative classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log_loss\n",
    "- known as Logistic loss or Cross-Entropy Loss\n",
    "- For a single sample with true label y∈{0,1} and a probability estimate p=Pr(y=1), the log loss is:\n",
    "\n",
    "        LOGLOSS(n=1)   =   ylog(p)+ (1-y)log(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Metrics like accuracy, precision, recall are good ways to evaluate classification models for balanced datasets, \n",
    "    but if the data is imbalanced then other methods like ROC/AUC perform better in evaluating the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
