{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d12466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42d4e8f1",
   "metadata": {},
   "source": [
    "df=pd.read_csv('https://github.com/amankharwal/US-presidents-heights/blob/f9cd6d65b8b4c484cddebe6197b8241f30a27a48/president_heights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b61e4",
   "metadata": {},
   "source": [
    "### Keyword Extraction using python Libraries"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5c6adb1",
   "metadata": {},
   "source": [
    "Keywords play an important role when reading a long text to understand the subject and context of the text. Search engines also analyze an article’s keywords before indexing it. In this article, I will walk you through how to extract keywords using Python."
   ]
  },
  {
   "cell_type": "raw",
   "id": "715a259a",
   "metadata": {},
   "source": [
    "There are so many Python libraries for the task of extracting keywords, the best ones are spaCy, Rake-Nltk, YAKE. In this tutorial, I will use the Rake-NLTK as it is beginner-friendly and easy to install. You can easily install it by using the pip command; pip install rake-nltk."
   ]
  },
  {
   "cell_type": "raw",
   "id": "353e8f81",
   "metadata": {},
   "source": [
    "RAKE stands for Rapid Automatic Keyword Extraction. It is only built to extract keywords by using the NLTK library in Python. Now let’s see how to use this library for extracting keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12983b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from rake-nltk) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake-nltk) (0.4.4)\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2fae017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d098a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake=Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7e221a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will store some text into a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f74c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" I am a programmer from India, and I am here to guide you \n",
    "with Data Science, Machine Learning, Python, and C++ for free. \n",
    "I hope you will learn a lot in your journey towards Coding, \n",
    "Machine Learning and Artificial Intelligence with me.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1b80eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake.extract_keywords_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98848670",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text=rake.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0196660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['journey towards coding', 'machine learning', 'machine learning', 'data science', 'c ++', 'artificial intelligence', 'python', 'programmer', 'lot', 'learn', 'india', 'hope', 'guide', 'free']\n"
     ]
    }
   ],
   "source": [
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e806837",
   "metadata": {},
   "source": [
    "### Text Summerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ed40456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cf85f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'T',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " ' ',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'S',\n",
       " 'u',\n",
       " 'm',\n",
       " 'm',\n",
       " 'a',\n",
       " 'r',\n",
       " 'i',\n",
       " 'z',\n",
       " 'e']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Enter! Text @ to Summarize.\"\n",
    "if text.count(\". \") > 20:\n",
    "    length = int(round(text.count(\". \")/10, 0))\n",
    "else:\n",
    "    length = 1\n",
    "\n",
    "nopuch =[x for x in text if x not in string.punctuation]\n",
    "nopuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7edd8a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enter Text  to Summarize'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopuch = \"\".join(nopuch)\n",
    "nopuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1c57c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = [x for x in nopuch.split() if x.lower() not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdde81a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Enter', 'Text', 'Summarize']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01d698e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Enter': 1, 'Text': 1, 'Summarize': 1}\n"
     ]
    }
   ],
   "source": [
    "word_freq = {}\n",
    "for x in processed_text:\n",
    "    if x not in word_freq:\n",
    "        word_freq[x] = 1\n",
    "    else:\n",
    "        word_freq[x] = word_freq[x] + 1\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "994a85ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_freq = max(word_freq.values())\n",
    "for word in word_freq.keys():\n",
    "    word_freq[word] = (word_freq[word]/max_freq)\n",
    "\n",
    "sent_list = nltk.sent_tokenize(text)\n",
    "sent_score = {}\n",
    "for sent in sent_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_freq.keys():\n",
    "            if sent not in sent_score.keys():\n",
    "                sent_score[sent] = word_freq[word]\n",
    "            else:\n",
    "                sent_score[sent] = sent_score[sent] + word_freq[word]\n",
    "\n",
    "summary_sents = nlargest(length, sent_score, key=sent_score.get)\n",
    "summary = \" \".join(summary_sents)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1771ca",
   "metadata": {},
   "source": [
    "### Real time sentiment analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9984dc46",
   "metadata": {},
   "source": [
    "Streamlit is an open-source Python framework used to deploy machine learning models in beautiful web applications in just a few lines of code. You don’t need any knowledge of web development to deploy machine learning models with this framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff021563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# Docstring: Give a sentiment intensity score to sentences.\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92230b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\iamvi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c61d9c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Rate Our Services >>: it is something confusing\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Please Rate Our Services >>: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "646243e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia=SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20b9f4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "score = sia.polarity_scores(user_input)\n",
    "if score[\"neg\"] != 0:\n",
    "      print(\"Negative\")\n",
    "else:\n",
    "      print(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9809b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "st.write(\"# Real Time Sentiment Analysis\")\n",
    "\n",
    "user_input = st.text_input(\"Please rate our services >>: \")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "s = SentimentIntensityAnalyzer()\n",
    "score = s.polarity_scores(user_input)\n",
    "\n",
    "if score == 0:\n",
    "    st.write(\" \")\n",
    "elif score[\"neg\"] != 0:\n",
    "    st.write(\"# Negative\")\n",
    "elif score[\"pos\"] != 0:\n",
    "    st.write(\"# Positive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
