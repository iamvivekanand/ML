{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Supervised Machine Learning algorithm can be broadly classified into Regression and Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Information Gain and how it is calculated?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-Information gain is the reduction in entropy or surprise by transforming a dataset and is often used in training decision trees. \n",
    "-Information gain is calculated by comparing the entropy of the dataset before and after a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classifier in Supervised ML?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "-The algorithm which implements the classification on a dataset is known as a classifier. \n",
    "\n",
    "There are two types of Classifications:\n",
    "1.Binary Classifier\n",
    "2.Multiclass classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Binary Classifier:\n",
    "-has only two possible outcomes,\n",
    "Multi-class Classifier: \n",
    "-has more than two outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are Learners in Classification Problems??"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Learners are algorirthm used to train the model, they learn from data\n",
    "\n",
    "There are two types of learners in classification\n",
    "1.Lazy Learner\n",
    "2.Eager Learner\n",
    "\n",
    "-lazy lerner takes more time in prediction\n",
    "-while an eager learner takes less time in prediction,\n",
    "-LAZY Learner and EAGER Learner are classified on the basis of time taken while doing prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe in detail about Lazy Learner?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lazy Learners:\n",
    "-stores the training dataset and wait until it receives the test dataset\n",
    "-classification is done on the basis of the most related data stored in the training dataset.\n",
    "-less time in training \n",
    "-more time for predictions\n",
    "-K-NN algorithm, Case-based reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe in detail about Eager Learner?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Eager Learners:\n",
    "-develop a classification model using training dataset before receiving a test dataset\n",
    "-more time in training\n",
    "-less time in prediction\n",
    "-e.g.  Decision Trees, Naïve Bayes, ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of ML Classification Algorithms:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification Algorithms can be further divided into the Mainly two category:\n",
    "1.Linear Models\n",
    "    Logistic Regression\n",
    "    Support Vector Machines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.Non-linear Models\n",
    "    K-Nearest Neighbours\n",
    "    Kernel SVM\n",
    "    Naïve Bayes\n",
    "    Decision Tree Classification\n",
    "    Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some evaluation metrics for regression??"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Mean square error\n",
    "- Root mean square error\n",
    "- f1 Score\n",
    "- R2 \n",
    "- adjusted R2\n",
    "- mean absoulte error\n",
    "- mean absolute percentage error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mean Absolute Error:\n",
    "-sum of average of the absolute difference between the predicted and actual values\n",
    "-MAE does not indicate the direction of the model i.e. no indication about underperformance or overperformance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mae.jpg' height=200 width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "1351079888211148.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error\n",
    "print(mean_absolute_error(y_test,y_pred))\n",
    "print(mean_absolute_percentage_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MEAN SQAURED ERROR:\n",
    "MSE is like the MAE, but the only difference is that the it squares the difference of actual and predicted output values before summing them all instead of using the absolute value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mse.jpg'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ROOT MEAN SQAURED ERROR:\n",
    "rmse=(mean_squared_error(y_test,y_pred))**1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics for a Classification model:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below are some famous metrics for classification model..\n",
    "1. confusion metrics\n",
    "2. accuracy score\n",
    "3. Classication Report (Precision,recall,f1-score,support)\n",
    "4. specificity\n",
    "5. roc_auc_score\n",
    "6. roc_curve\n",
    "7. auc (area under roc curve)\n",
    "8. f1_score \n",
    "9. log_los (logarithmic loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classification Accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ratio of Number of correct predictions made to the all predictions made\n",
    "\n",
    "    accuracy   =   TP+TN / TP+TN+FP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classification report"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This report consists of the scores of Precisions, Recall, F1 and Support"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision: mostly used in documents retrieval, defined as no of correct documents retrieved by the model to the total correct document in the queried data\n",
    "\n",
    "TP/TP+FP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recall or Sensitivity:\n",
    "\n",
    "TP/TP+FN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Specificity:\n",
    "Specificity, in contrast to recall, may be defined as the number of negatives returned by our ML model\n",
    "\n",
    "Specificity=  TN/TN+FP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Support:\n",
    "Support may be defined as the number of samples of the true response that lies in each class of target values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "F1 Score:\n",
    "-This score will give us the harmonic mean of precision and recall. \n",
    "-Mathematically, F1 score is the weighted average of the precision and recall. \n",
    "-The best value of F1 would be 1 and worst would be 0\n",
    "\n",
    "𝑭𝟏 = 𝟐 ∗ (𝒑𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏 ∗ 𝒓𝒆𝒄𝒂𝒍𝒍) / (𝒑𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏 + 𝒓𝒆𝒄𝒂𝒍𝒍)\n",
    "\n",
    "from sklearn.metrics import 𝑭𝟏_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Log Loss/logarithmic loss/Cross-Entropy Loss:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-also called Logistic regression loss or cross-entropy loss.\n",
    "-Used for a classifier whose output is a probability value between the 0 and 1.\n",
    "-Lower log loss higher accuracy\n",
    "-For a good binary Classification model, the value of log loss should be near to 0.\n",
    "\n",
    "As we know that accuracy is the count of predictions (predicted value = actual value) in our model whereas Log Loss is the amount of uncertainty of our prediction based on how much it varies from the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets us see how we can calculate all these score\n",
    "\n",
    "from sklearn.metrics import log_loss,accuracy_score,f1_score,auc,roc_auc_score,roc_curve,precision_score,recall_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "y_pred = [1, 0, 1, 1, 1, 0, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.815750437193334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60         6\n",
      "           1       0.50      0.75      0.60         4\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.62      0.62      0.60        10\n",
      "weighted avg       0.65      0.60      0.60        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Or simply we can print all these values using classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Confusion Matrix:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-Known as the error matrix.\n",
    "-Consists predictions in summarized form, it has a total number of correct predictions and incorrect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='confusion-matrix.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. AUC (area under the roc_curve):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-ROC: Receiver Operating Characteristics Curve \n",
    "-AUC: Area Under the Curve.\n",
    "-we use the AUC-ROC Curve for visualizing the performance of the multi-class classification model\n",
    "-it shows the performance of the classification model at different thresholds"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The ROC curve is plotted with TPR (True Positive Rate) on Y-axis and FPR(False Positive Rate) on X-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roc_curve(y_test,y_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
