{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7059a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and Recall in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec914ec4",
   "metadata": {},
   "source": [
    "Precision is the ratio between the True Positives and to the all positive cases\n",
    "\n",
    "    Precision=TP/(TP+FP)\n",
    "\n",
    "If Precision for our model is 0.843 or, when it predicts that a patient has heart disease, it is correct around 84% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edc5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "### What is recall in Model Evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a17a8",
   "metadata": {},
   "source": [
    "Ration of True Positive to the actual Positive..\n",
    "\n",
    "    Recall=TP/(TP+FN)  -- here FN means it was also positive but our model predicted it negative thus false negative. \n",
    "\n",
    "Recall also gives a measure of how accurately our model is able to identify the relevant data. We refer to it as Sensitivity or True Positive Rate\n",
    "\n",
    "\n",
    "if recall is high, that would mean model is more likely to predict the patient doesn't have a heart disease but he has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc8e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42135fc2",
   "metadata": {},
   "source": [
    "Ratio of the total number of correct predictions and the total number of predictions\n",
    "\n",
    "    Accuracy= TP+TN/(TP+TN+FP+FN)\n",
    "\n",
    "We need a tradeoff between Precision and Recall. We first need to decide which is more important for our classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51a8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is f1-score? when do we need it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24508d",
   "metadata": {},
   "source": [
    "When precision and recall are equally important for us, than we need to find f1-score, since it is the harmonnic mean of precision and recall..\n",
    "\n",
    "    f1=score= 2 * (precision x recall)/(precision+recall)\n",
    "\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1_score(y_test,y_pred)\n",
    "\n",
    "We can also compute it using classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00dd907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ROC CURVE??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab4894",
   "metadata": {},
   "source": [
    "**FALSE POSITIVE RATE:**\n",
    "\n",
    "Ratio of the False Positives to the Actual number of Negatives.\n",
    "\n",
    "True Negative Rate: Ratio of the True Negatives and the Actual Number of Negatives.\n",
    "\n",
    "TNR = 1 – FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc769df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. ROC Curves(Receiver Operating Characteristic Curve):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e630a5d",
   "metadata": {},
   "source": [
    "It is the plot between the TPR(y-axis) and FPR(x-axis). Since our model classifies the patient as having heart disease or not based on the probabilities generated for each class, we can decide the threshold of the probabilities as well.\n",
    "\n",
    "For example, we want to set a threshold value of 0.4. This means that the model will classify the datapoint/patient as having heart disease if the probability of the patient having a heart disease is greater than 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61cd028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When do we Need high precision or high recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6e027",
   "metadata": {},
   "source": [
    "Models need high recall when you need output-sensitive predictions. For example, predicting cancer or predicting terrorists needs a high recall, in other words, you need to cover false negatives as well. It is ok if a non-cancer tumor is flagged as cancerous but a cancerous tumor should not be labeled non-cancerous.\n",
    "\n",
    "Similarly, we need high precision in places such as recommendation engines, spam mail detection, etc. Where you don’t care about false negatives but focus more on true positives and false positives. It is ok if spam comes into the inbox folder but a really important mail shouldn’t go into the spam folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae84578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
