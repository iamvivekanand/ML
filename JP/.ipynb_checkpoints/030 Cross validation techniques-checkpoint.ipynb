{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique for validating the model efficiency by training it on the subsets of input data and testing using an unseen subset of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the basic steps of In all cross-validations techniques are:\n",
    "\n",
    "* Reserve some portion of the dataset as a validation set.\n",
    "* do model tranining with rest data\n",
    "* Evaluate model performance using the validation set. \n",
    "* If the model performs well with the validation set, go with the further step, else check for the issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some methods of Cross validation techniques, Discuss some of Cross validation techniques??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common methods that are used for cross-validation:\n",
    "- Validation Set Approach\n",
    "- Holdout method\n",
    "- Leave-P-out cross-validation\n",
    "- Leave one out cross-validation\n",
    "- K-fold cross-validation\n",
    "- Stratified k-fold cross-validation\n",
    "- Repeated K-folds\n",
    "- Nested K-folds\n",
    "- Time series CV   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We devide our dataset into training and test dataset(validation data), both are given 50 % part\n",
    "\n",
    "The big disadvantages is that we are just using a 50% of dataset to train the model, so the model may miss out to capture important information of the dataset. It may result in underfitting.\n",
    "\n",
    "With the development of finer CV techniques, We just keep it for knowledge and use the best of available techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.holdout method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simplest and most common technique.\n",
    "- Divide the dataset into two parts,the training set and the test set. Usually, 80% of the dataset goes to the training set and 20% to the test set but you may choose any splitting that suits you better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('titanic-train.csv')\n",
    "X=df.drop('Survived',axis=1)\n",
    "y=df.Survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disadvantages of holdout method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a dataset that is not completely evenly distributed, traning dataset might be imbalanced, might not represent the dataset accurately to the model\n",
    "- traning and test data might be completely different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Leave-P-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    If there are total N datapoints in the original input dataset, \n",
    "    Then N-P data points will be used as the training dataset and the p data points as the validation set. \n",
    "\n",
    "    This complete process is repeated for all the samples, and the average error is calculated to know the effectiveness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "# X = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20]])\n",
    "# y = np.array([1, 2])\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset('tips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>size</th>\n",
       "      <th>sex_Female</th>\n",
       "      <th>smoker_No</th>\n",
       "      <th>day_Fri</th>\n",
       "      <th>day_Sat</th>\n",
       "      <th>day_Sun</th>\n",
       "      <th>time_Dinner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip  size  sex_Female  smoker_No  day_Fri  day_Sat  day_Sun  \\\n",
       "0       16.99  1.01     2           1          1        0        0        1   \n",
       "1       10.34  1.66     3           0          1        0        0        1   \n",
       "\n",
       "   time_Dinner  \n",
       "0            1  \n",
       "1            1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.get_dummies(df,drop_first=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('total_bill',axis=1)\n",
    "y=df.total_bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpo=LeavePOut(2)\n",
    "# p : int,Size of the test sets. Must be strictly less than the number of samples\n",
    "\n",
    "# for train_index,test_index in lpo.split(X):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "\n",
    "# for train_index, test_index in loo.split(X):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Leave one out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar to leave p-out cross validation\n",
    "- here we leave only one data point for validation\n",
    "- This process is repeated for each datapoint. Hence for n samples, we get n different training set and n different test set\n",
    "\n",
    "It has the following features:\n",
    "- bias is minimum as all the data points are used \n",
    "- process is executed for n times; hence execution time is high.\n",
    "- high variation in testing the effectiveness of the model as we iteratively check against one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5]]\n",
      "[[ 6  7  8  9 10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = np.array([[1,2,3,4,5], [6,7,8,9,10]])\n",
    "y = np.array([1, 2])\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data bias in machine learning is a type of error in which certain elements of a dataset are more heavily weighted and/or represented than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input dataset is devided into k number of sample, each of equal size\n",
    "- use k-1 sample for training, and 1 for testing\n",
    "- repeat for all sample\n",
    "- take mean of all performance estimates(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=2)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "skf = StratifiedKFold(n_splits=2)     #n_splits : int, default=5, Number of folds. Must be at least 2\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Repeated k-Fold cross-validation\n",
    "\n",
    "- Repeated random sub-sampling CV, \n",
    "- is probably the most robust of all CV techniques.\n",
    "- its a variation of k-fold\n",
    "- k is not the number of folds,It is the number of times we will train the model\n",
    "\n",
    "The general idea is that on every iteration we will randomly select samples all over the dataset as our test set. For example, if we decide that 20% of the dataset will be our test set, 20% of samples will be randomly selected and the rest 80% will become the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Accuracy: 100.0 %\n",
      "\n",
      "Minimum Accuracy: 87.5 %\n",
      "\n",
      "Overall Accuracy: 95.2515664160401 %\n",
      "\n",
      "Standard Deviation is: 0.028043703637038998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedKFold as RKF\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "# Since max_iter is 100 by default and stratifiedKFold need to iter again and again\n",
    "\n",
    "rkf= RKF(n_splits=10,random_state=41)\n",
    "mylist = []\n",
    "\n",
    "for train_index, test_index in rkf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lr.fit(x_train, y_train)\n",
    "    mylist.append(lr.score(x_test, y_test))\n",
    "    \n",
    "print('\\nMaximum Accuracy:',max(mylist)*100, '%')\n",
    "print('\\nMinimum Accuracy:',min(mylist)*100, '%')\n",
    "print('\\nOverall Accuracy:',np.mean(mylist)*100, '%')\n",
    "print('\\nStandard Deviation is:', np.std(mylist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.Stratified k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar to k-fold cross-validation with some little changes. \n",
    "- works on stratification concept i.e. rearranges the data to ensure that each fold or group is a good representative of the complete dataset, as we may face a large imbalance of the target value in the dataset\n",
    "- To deal with the bias and variance, it is one of the best approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x = cancer.data                        \n",
    "y = cancer.target     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of possible accuracy: [0.9649122807017544, 0.9298245614035088, 0.9473684210526315, 0.9473684210526315, 0.9122807017543859, 0.9649122807017544, 0.9649122807017544, 0.9473684210526315, 0.9649122807017544, 0.9821428571428571]\n",
      "\n",
      "Maximum Accuracy: 98.21428571428571 %\n",
      "\n",
      "Minimum Accuracy: 91.22807017543859 %\n",
      "\n",
      "Overall Accuracy: 95.26002506265667 %\n",
      "\n",
      "Standard Deviation is: 0.019249997664881075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "# Since max_iter is 100 by default and stratifiedKFold need to iter again and again\n",
    "\n",
    "skf = SKF(n_splits=10, shuffle=True, random_state=1)\n",
    "mylist = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lr.fit(x_train, y_train)\n",
    "    mylist.append(lr.score(x_test, y_test))\n",
    "    \n",
    "print('List of possible accuracy:', mylist)\n",
    "print('\\nMaximum Accuracy:',max(mylist)*100, '%')\n",
    "print('\\nMinimum Accuracy:',min(mylist)*100, '%')\n",
    "print('\\nOverall Accuracy:',np.mean(mylist)*100, '%')\n",
    "print('\\nStandard Deviation is:', np.std(mylist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What is the difference between Cross-validation to train/test split in Machine Learning??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Train/test split: \n",
    "    The input data is divided into two parts, training set and test set in a ratio of 70:30, 80:20, etc. \n",
    "    It provides a high variance, which is one of the biggest disadvantages.\n",
    "    \n",
    "    While Cross Validation is used to overcome the disadvantage of train/test split by splitting the dataset into groups, training on each, and then averaging the result\n",
    "\n",
    "    It is more efficient as as every observation is used for the training and testing both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What are some Limitations of Cross-Validation? Is there any limitation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ideal conditions, it provides the optimum output. But for the inconsistent data, it may produce a drastic result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the END of the Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
