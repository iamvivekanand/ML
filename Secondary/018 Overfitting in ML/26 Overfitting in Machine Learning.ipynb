{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in ML Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A statistical model is said to be overfitted if it canâ€™t generalize well with unseen data.\n",
    "- possible causes of overfitting are impurities, noisy data, outliers, missing data, or imbalanced data etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some terms related to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise: \n",
    "Noise is meaningless or irrelevant data present in the dataset. It affects the performance of the model if it is not removed.\n",
    "\n",
    "Bias: \n",
    "it is error in the prediction, or simply it is the difference in Predicted values and Actual Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model works well with unseen data, then it is said to be well Generalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the tradeoff between bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and variance are complements of each other. The increase of one will result in the decrease of the other and vice versa. Hence, finding the right balance of values is known as the Bias-Variance Tradeoff. An ideal algorithm should neither underfit nor overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causes of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting occurs when the model fits more data than required, and it tries to capture each and every datapoint fed to it. Hence it starts capturing noise and inaccurate data from the dataset, which degrades the performance of the model.\n",
    "\n",
    "- For example, if the model shows 85% accuracy with training data and 50% accuracy with the test dataset, it means the model is not performing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An overfitted model has low bias and high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ways to aovide overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some popular ways to avoid overfitting in ML models\n",
    "- Early Stopping\n",
    "- Train with more data\n",
    "- Feature Selection\n",
    "- Cross-Validation\n",
    "- Data Augmentation\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping:\n",
    "- In this training is paused before the model starts learning the noise. \n",
    "- We measure the performance of the model after each iteration of model training. \n",
    "- keep training up to a certain no of iteration, until a new iteration improves the performance of the model.\n",
    "- If model begins to overfit after this, stop the process\n",
    "- It may cause underfitting problem if training is paused too early. So, we need to find a \"sweet spot\" between underfitting and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with More data:\n",
    "- it can enhance chances of capturing more information\n",
    "- simultaneously more data can also lead a model to capture irrelavant, inconsistent and noisy data\n",
    "- so we must make sure that the data is well cleaned and appropriate for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection:\n",
    "- it is way of selection the most contributing features and removing less affecting features\n",
    "- redundant and less important features are removed\n",
    "- some model comes with inbuilt feature selection, otherwise we can do this manually\n",
    "- there are various features selection techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation:\n",
    "- in this techniques, we reserve a part of original dataset for testing the model and rest part is utilized for traning\n",
    "- some examples are: k-fold CV, stratified K-fold CV, Holdout method etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation:\n",
    "- it is a data analysis technique\n",
    "- it is an alternative of adding more data to avoid overfitting\n",
    "- We simply modify the original data instead of adding more data\n",
    "- it makes data sample slightly different every time it is processed by the model. Hence each data set appears unique to the model and prevents overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization:\n",
    "- it is a form of regression, that regularizes or shrinks the coefficient estimates towards zero.\n",
    "- adds an additional penalty term in the error function or cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
