{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix in Machine Learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-matrix used to determine the performance of the classification models\n",
    "-also called error matrix\n",
    "-For 2 prediction classes, matrix is a 2*2 table, for 3 classes, it is 3*3 table, and so on"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The matrix is divided into two dimensions, that are predicted values and actual values along with the total number of predictions.\n",
    "\n",
    "Predicted values are values predicted by the model, \n",
    "and actual values are the true values for the given observations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The matrix has following cases:\n",
    "\n",
    "True Negative: predicted No, actual No.\n",
    "\n",
    "True Positive: predicted yes, actual yes\n",
    "\n",
    "False Negative: predicted no, actual Yes, \n",
    "((Type-II error))\n",
    "\n",
    "False Positive: Predicted Yes, Actual No. \n",
    "((Type-I error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the use or Need of Confusion Matrix?? Describe the role of Confusion matrix in determining Model performances"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It evaluates the performance of the classification models by comparing Predicted values and Actual values of Test Data\n",
    "\n",
    "We can calculate the different parameters for the model, such as accuracy, precision, etc. using Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculations using Confusion Matrix:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification Accuracy:\n",
    "Tells accuracy of classificatin models, i.e. tells how often a model predicts correctly.\n",
    "\n",
    "The formula for accuracy is given below\n",
    "TP+TN/TP+TN+FP+FN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Misclassification rate (Error Rate):\n",
    "-defines how often the model gives the wrong predictions.\n",
    "-ratio of INCORRECT predictions to ALL PREDICTION made by the classifier\n",
    "\n",
    "FP+FN/TP+TN+FP+FN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision:\n",
    "Ratio of true postive and all positive classed predicted by the model\n",
    "\n",
    "TP/TP+FP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "TP/TP+FN\n",
    "Recall must be as high as possible."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "F-measure:\n",
    "If two models have low precision and high recall or vice versa, it is difficult to compare these models. So, for this purpose, we can use F-score.\n",
    "It helps us to evaluate the recall and precision at the same time\n",
    "\n",
    "The F-score is maximum if the recall is equal to the precision\n",
    "\n",
    "2*recall*precision/recall+precision"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Null Error rate:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ROC Curve:\n",
    "The ROC is a graph displaying a classifier's performance for all possible thresholds. \n",
    "The graph is plotted between the true positive rate (on the Y-axis) and the false Positive rate (on the x-axis)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
