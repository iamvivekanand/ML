{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression in Machine Learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-it shows linear relationship between i/p and o/p variable\n",
    "-best fit line is a straight line, it is sloped at some angle also\n",
    "-it finds how the value of the dependent variable is changing according to the value of the independent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='lr.png' height=250 width=300>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mathematically, we can represent a linear regression as:  \n",
    "y= a0+a1x+ ε\n",
    "WHERE:\n",
    "    Y= Dependent Variable (Target Variable)\n",
    "    X= Independent Variable (predictor Variable)\n",
    "    a0= intercept of the line (Gives an additional degree of freedom)\n",
    "    a1 = Linear regression coefficient (scale factor to each input value).\n",
    "    ε = random error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Simple Linear Regression:\n",
    "-sinle i/p variable is used to predict dependent variable\n",
    "\n",
    "Multiple Linear regression:\n",
    "-more than one i/p variable is used to predict dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Line"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-best fit line showing the realtionship between dependent and independent variable\n",
    "\n",
    "A Linear Regression Line can show two types of relationship:\n",
    "1.Positive Linear Relationship: value of output increases on Y-axis with value of input on X-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='positivelr.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.Negative Linear Relationship:If the dependent variable decreases with incrase in indepedent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='negativelr.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Find the best fit line in Linear Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The different values for weights or the coefficient of lines (a0, a1) gives a different line of regression, so we need to calculate the best values for a0 and a1 to find the best fit line, so to calculate this we use cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function-"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-cost function is used to estimate the values of the coefficient for the best fit line.\n",
    "-Cost function optimizes the regression coefficients or weights\n",
    "\n",
    "For Linear Regression, we use the Mean Squared Error (MSE) cost function, which is the average of squared error occurred between the predicted values and actual values. It can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mseforlr.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Where,\n",
    "N=Total number of observation\n",
    "Yi = Actual value\n",
    "(a1xi+a0)= Predicted value."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Residuals: The distance between the actual value and predicted values is called residual\n",
    "if residual is high, cost function will be high, and low when low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gradient descent is used to minimize cost function (MSE in case of Linear Regression)\n",
    "a linear model uses gradient descent to update the coefficient of the line\n",
    "\n",
    "It is done by a random selection of values of coefficient and then iteratively update the values to reach the minimum cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The process of finding the best model out of various models is called optimization. It can be achieved by below method:\n",
    "\n",
    "R-squared method:\n",
    "-statistical method to determine the foodness of git\n",
    "-It measures the strength of the relationship between the dependent and independent variables on a scale of 0-100%.\n",
    "-The high value of R-square determines the less difference between the predicted values and actual values and hence represents a good model.\n",
    "-It is also called a coefficient of determination, or coefficient of multiple determination for multiple regression.\n",
    "-It can be calculated from the below formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='rsqaure.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumptions of Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Linear relationship between the features and target:\n",
    "-Linear regression assumes the linear relationship between the dependent and independent variables.\n",
    "\n",
    "Small or no multicollinearity between the features:\n",
    "-Multicollinearity means high-correlation between the independent variables. Due to multicollinearity, it may difficult to find the true relationship between the predictors and target variables. Or we can say, it is difficult to determine which predictor variable is affecting the target variable and which is not. So, the model assumes either little or no multicollinearity between the features or independent variables.\n",
    "\n",
    "Homoscedasticity Assumption:\n",
    "-Homoscedasticity is a situation when the error term is the same for all the values of independent variables, With homoscedasticity, there should be no clear pattern distribution of data in the scatter plot.\n",
    "\n",
    "Normal distribution of error terms:\n",
    "-the error term should follow the normal distribution pattern. If error terms are not normally distributed, then confidence intervals will become either too wide or too narrow, which may cause difficulties in finding coefficients.\n",
    "It can be checked using the q-q plot. If the plot shows a straight line without any deviation, which means the error is normally distributed.\n",
    "\n",
    "No autocorrelations:\n",
    "-no autocorrelation in error terms. If there will be any correlation in the error term, then it will drastically reduce the accuracy of the model. Autocorrelation usually occurs if there is a dependency between residual errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
