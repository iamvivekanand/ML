{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- its a supervised ML algorithm, Can be used for Regression & Classification but mostly it is used for the Classification problems. \n",
    "- Non-parametric means that there is no assumption for the underlying data distribution\n",
    "- it categorizes a new data into the category that is most similar to the available categories\n",
    "- KNN  stores all the available data and classifies a new data based on the similarity.\n",
    "- KNN is a lazy learner algorithm, It means it takes less time in training and takes more time for prediction.\n",
    "\n",
    "The module, **sklearn.neighbors** that implements the k-nearest neighbors algorithm, provides the functionality for **unsupervised** as well as **supervised neighbors**-based learning methods\n",
    "\n",
    "The unsupervised nearest neighbors implement different algorithms (BallTree, KDTree or Brute Force) to find the nearest neighbor(s) for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need a K-NN Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we need knn algorithm To identify the category or class of a datapoint, based on avaiable dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='knn.jpg'>\n",
    "image source:Javatpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does K-NN work? Describe the steps involve in KNN Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step-1: Select K numbers of neighbors around the new point\n",
    "    Step-2: Calculate the Euclidean distance of K neighbors \n",
    "    Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n",
    "    Step-4: Count the number of the data points in each category, for each neighbors\n",
    "    Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n",
    "    Step-6: Our model is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the below figure and we have to classify the new datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='knn1.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will choose the number of neighbors, so we will choose the k=5.\n",
    "Next, we will calculate the Euclidean distance between the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='euclidian-distance2.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        Image Source:javatpoint.com    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the Euclidean distance we got the nearest neighbors, as three nearest neighbors in category A and two nearest neighbors in category B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='knn2.jpg'>\n",
    "\n",
    "                                        Image Source:javatpoint.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to select the value of K neighbors in the K-NN Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no particular way to determine the best value for \"K\", we start by taking some random values of K. The most preferred value for K is 5.\n",
    "- A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.\n",
    "    Large values for K are good, but it may find some difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A low value of K causes a highly complex model as well, which might result in over-fitting\n",
    "    of the model.\n",
    "- It means the prediction process is not generalized enough to be used for out-of-sample cases.\n",
    "- Out-of-sample data is the data that is outside of the dataset used to train the model.\n",
    "\n",
    "Now, on the opposite side of the spectrum, if we choose a very high value of K, such\n",
    "as K=20, then the model becomes overly generalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanctages of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- robust to the noisy training data\n",
    "- more effective if the training data is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disvantages of knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Always needs to determine the value of K which may be complex some time.\n",
    "- The computation cost is high because of calculating the distance between the data points for all the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python implementation of the KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=sns.load_dataset('iris')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,0:4].values\n",
    "y=df.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X),type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species=le.fit_transform(df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'setosa', 'setosa'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2) \n",
    "\n",
    "# here p is power of distance metric of minikowski\n",
    "# When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. \n",
    "# For arbitrary p, minkowski_distance (l_p) is used.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= knn.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  1.0\n",
      "f1_score:  1.0\n",
      "precision score:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, r2_score,f1_score,precision_score,recall_score,roc_auc_score\n",
    "print('accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "# print('r2 score: ',r2_score(y_test,y_pred))\n",
    "print('f1_score: ',f1_score(y_test,y_pred,average='weighted'))\n",
    "print('precision score: ',precision_score(y_test,y_pred,average='micro'))\n",
    "# print('recall score: ',recall_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  0,  0],\n",
       "       [ 0, 13,  0],\n",
       "       [ 0,  0,  6]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Question Regarding KNN Algorithm??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is mean by \"knn is a non-parametric algorithm\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it doesn’t assume anything about the underlying data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROS:\n",
    "- It is very simple algorithm to understand and interpret\n",
    "- It is very useful for nonlinear data because there is no assumption about data in this algorithm\n",
    "- It is a versatile algorithm as we can use it for classification as well as regression\n",
    "- It has relatively high accuracy but there are much better supervised learning models than KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONS:\n",
    "- It is computationally a bit expensive algorithm because it stores all the training data\n",
    "- High memory storage required as compared to other supervised learning algorithms\n",
    "- Prediction is slow in case of big N\n",
    "- It is very sensitive to the scale of data as well as irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=5,\n",
    "                        weights='uniform',\n",
    "                        algorithm='auto',\n",
    "                        leaf_size=30,\n",
    "                        p=2,\n",
    "                        metric='minkowski',\n",
    "                        metric_params=None,\n",
    "                        n_jobs=None,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "    n_neighbors : int, default=5\n",
    "        Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "\n",
    "    weights : {'uniform', 'distance'} or callable, default='uniform'\n",
    "        weight function used in prediction.  Possible values:\n",
    "\n",
    "        - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "          are weighted equally.\n",
    "        - 'distance' : weight points by the inverse of their distance.\n",
    "          in this case, closer neighbors of a query point will have a\n",
    "          greater influence than neighbors which are further away.\n",
    "        - [callable] : a user-defined function which accepts an\n",
    "          array of distances, and returns an array of the same shape containing the weights.\n",
    "          \n",
    "              \n",
    "    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
    "        Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "        - 'ball_tree' will use :class:`BallTree`\n",
    "        - 'kd_tree' will use :class:`KDTree`\n",
    "        - 'brute' will use a brute-force search.\n",
    "        - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method.\n",
    "\n",
    "        Note: fitting on sparse input will override the setting of this parameter, using brute force."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    leaf_size : int, default=30\n",
    "    Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory\n",
    "    required to store the tree.  The optimal value depends on the nature of the problem.\n",
    "        \n",
    "    p : int, default=2\n",
    "        Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), \n",
    "        and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "        \n",
    "    metric : str or callable, default='minkowski'\n",
    "    the distance metric to use for the tree.  The default metric is\n",
    "    minkowski, and with p=2 is equivalent to the standard Euclidean\n",
    "    metric. See the documentation of :class:`DistanceMetric` for a\n",
    "    list of available metrics.\n",
    "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
    "    must be square during fit. X may be a :term:`sparse graph`,\n",
    "    in which case only \"nonzero\" elements may be considered neighbors.\n",
    "\n",
    "    metric_params : dict, default=None\n",
    "        Additional keyword arguments for the metric function.\n",
    "\n",
    "    n_jobs : int, default=None\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "        Doesn't affect :meth:`fit` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "    classes_ : array of shape (n_classes,)\n",
    "        Class labels known to the classifier\n",
    "\n",
    "    effective_metric_ : str or callble\n",
    "        The distance metric used. It will be same as the `metric` parameter\n",
    "        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
    "        'minkowski' and `p` parameter set to 2.\n",
    "\n",
    "    effective_metric_params_ : dict\n",
    "        Additional keyword arguments for the metric function. For most metrics\n",
    "        will be same with `metric_params` parameter, but may also contain the\n",
    "        `p` parameter value if the `effective_metric_` attribute is set to\n",
    "        'minkowski'.\n",
    "\n",
    "    n_samples_fit_ : int\n",
    "        Number of samples in the fitted data.\n",
    "\n",
    "    outputs_2d_ : bool\n",
    "        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
    "        otherwise True.\n",
    "\n",
    "\n",
    "    >>> print(knn.predict([[1.1]]))\n",
    "    [0]\n",
    "    >>> print(knn.predict_proba([[0.9]]))\n",
    "    [[0.66666667 0.33333333]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised KNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module, **sklearn.neighbors** that implements the k-nearest neighbors algorithm, provides the functionality for unsupervised as well as supervised neighbors-based learning methods.\n",
    "\n",
    "**sklearn.neighbors.NearestNeighbors** is the module used to implement Unsupervised KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter & Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    n_neighbors:\n",
    "    The number of neighbors to get, default 5,int, optional\n",
    "\t\n",
    "    radius:\n",
    "    distance limit to find neighbors,default 1.0, float,optional \t\n",
    "    \n",
    "    algorithm:\n",
    "    {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional\n",
    "    Algorithm to compute the nearest neighbors, If ‘auto’, it will decide the most appropriate algorithm based on the values       passed to fit method.\n",
    "    \n",
    "    leaf_size:\n",
    "    It is passed to BallTree or KDTree, affect the speed of the construction & query, default 30.\n",
    "    \n",
    "    metric:\n",
    "    metric to use to compute distance between points.\n",
    "    We can choose from metric from scikit-learn or scipy.spatial.distance.\n",
    "    Scikit-learn − [‘cosine’,’manhattan’,‘Euclidean’, ‘l1’,’l2’, ‘cityblock’]\n",
    "    \n",
    "    Scipy.spatial.distance- \n",
    "    [‘braycurtis’,‘canberra’,‘chebyshev’,‘dice’,‘hamming’,‘jaccard’,‘correlation’,‘kulsinski’,\n",
    "    ‘mahalanobis’,‘minkowski’,‘rogerstanimoto’,‘russellrao’, ‘sokalmicheme’,'sokalsneath','seuclidean','sqeuclidean','yule']\n",
    "    Default is minkowski>>\n",
    "    \n",
    "    P:\n",
    "    Parameter for the Minkowski metric\n",
    "    Default 2, which is equivalent to using Euclidean_distance(l2)\n",
    "    \n",
    "    N_jobs:\n",
    "    numer of parallel jobs to run for neighbor search. The default value is None, int, optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 3],\n",
       "       [1, 2, 0],\n",
       "       [2, 1, 0],\n",
       "       [3, 4, 0],\n",
       "       [4, 5, 3],\n",
       "       [5, 6, 4],\n",
       "       [6, 5, 4]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "X = np.array([[-1, 1], [-2, 2], [-3, 3], [1, 2], [2, 3], [3, 4],[4, 5]])\n",
    "nn = NearestNeighbors(n_neighbors = 3, algorithm = 'ball_tree')\n",
    "nn.fit(X)\n",
    "\n",
    "# Now, find the K-neighbors of data set. It will return the indices and distances of the neighbors of each point.\n",
    "distances, indices = nn.kneighbors(X)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.41421356, 2.23606798],\n",
       "       [0.        , 1.41421356, 1.41421356],\n",
       "       [0.        , 1.41421356, 2.82842712],\n",
       "       [0.        , 1.41421356, 2.23606798],\n",
       "       [0.        , 1.41421356, 1.41421356],\n",
       "       [0.        , 1.41421356, 1.41421356],\n",
       "       [0.        , 1.41421356, 2.82842712]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.kneighbors_graph(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :4]\n",
    "y = iris.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# \n",
    "range_k = range(1,15)\n",
    "scores = {}\n",
    "scores_list = []\n",
    "for k in range_k:\n",
    "   classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "   classifier.fit(X_train, y_train)\n",
    "   y_pred = classifier.predict(X_test)\n",
    "   scores[k] = metrics.accuracy_score(y_test,y_pred)\n",
    "   scores_list.append(metrics.accuracy_score(y_test,y_pred))\n",
    "result = metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[20  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 10]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = metrics.classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Year of Birth: 1993\n",
      "Your age is 29 years\n"
     ]
    }
   ],
   "source": [
    "x=int(input('Enter your Year of Birth: '))\n",
    "import datetime as dt\n",
    "y=int('2022')\n",
    "z=y-x\n",
    "print('Your age is', z, 'years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
