{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5982f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the K-Nearest Neighbors algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562c80b",
   "metadata": {},
   "source": [
    "A supervised algorithm used for classification and regression tasks. It classifies a new data point based on the majority class of its K nearest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5300baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the KNN algorithm work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd43d2",
   "metadata": {},
   "source": [
    "The KNN algorithm works by calculating the distances between the new data point and all other data points in the dataset. Then, it selects the K nearest neighbors based on these distances. For classification, it assigns the majority class among these neighbors to the new data point. For regression, it takes the average (or weighted average) of the target values of these neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39bf49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the key hyperparameters of the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e923ada",
   "metadata": {},
   "source": [
    "The main hyperparameter of the KNN algorithm is 'K', which represents the number of nearest neighbors to consider. \n",
    "and **Distance metric** used for calculating distances between data points, such as Euclidean distance, Manhattan distance, or cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abab254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do you choose the value of 'K' in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53851c58",
   "metadata": {},
   "source": [
    "The value of 'K' in KNN should be chosen carefully as it directly affects the performance of the algorithm. A small value of 'K' can lead to noisy classifications, while a large value of 'K' can smooth out decision boundaries and may lead to misclassification of data points from different classes. Typically, 'K' is chosen through cross-validation, where different values of 'K' are tested, and the one that gives the best performance on validation data is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "007de297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the Advantages and Disadvantages of the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d420cbd5",
   "metadata": {},
   "source": [
    "**Advantages:** \n",
    "- easy to understand and implement, \n",
    "- requires no training phase, and \n",
    "- can be used for both classification and regression tasks. \n",
    "- It also works well with small datasets and is robust to noisy data.\n",
    "\n",
    "**Disadvantages:** \n",
    "\n",
    "- can be computationally expensive, especially for large datasets, as it requires calculating distances between the new data point and all other data points in the dataset. \n",
    "- sensitive to the choice of distance metric and the value of 'K', and it does not perform well with high-dimensional data due to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64847b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does KNN handle categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df748a4",
   "metadata": {},
   "source": [
    "by converting them into numerical values using techniques like one-hot encoding or label encoding, then calculates distances accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4d1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are some techniques to improve the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc599f",
   "metadata": {},
   "source": [
    "**Feature scaling:**\n",
    "Normalizing or standardizing the features to ensure that all features contribute equally to distance calculations.\n",
    "\n",
    "**Dimensionality reduction:** Using techniques like Principal Component Analysis (PCA) to reduce the dimensionality of the feature space and mitigate the curse of dimensionality.\n",
    "\n",
    "**Ensemble methods:** Combining multiple KNN models or using techniques like bagging or boosting to improve predictive performance.\n",
    "\n",
    "**Distance weighting:** Assigning weights to neighbors based on their distance to the new data point, giving more weight to closer neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27df7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
