{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- its a SML techniques, its a type of classification algorithm\n",
    "- but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n",
    "- Instead of fitting a regression line, we fit an \"S\" shaped logistic function, which predicts two maximum values (0 or 1).\n",
    "- This S shaped curve indicate Sigmoid function or logistic function\n",
    "- maps any real value within a range of 0 and 1.\n",
    "- In logistic regression, we use the concept of the threshold value,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='logistic-equation.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        Image source: Javatpoint.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How may types of Logistic Regression are there??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Logistic Regression can be classified into three types:\n",
    "    Binomial LR: \n",
    "    only two possible types of output, e.g. 0 or 1, Pass or Fail, etc.\n",
    "\n",
    "    Multinomial LR:\n",
    "    3 or more possible ouputs, but there r not ordered \n",
    "    such as \"cat\", \"dogs\", or \"sheep\"\n",
    "\n",
    "    Ordinal LR: \n",
    "    3 or more possible output, but they are ordered. Such as low,Medium,High, OR good, better and best etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=sns.load_dataset('iris')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,0:4].values\n",
    "y=df.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species=le.fit_transform(df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesuring accuracy of the model\n",
    "\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, \n",
      "          one of [None, 'micro', 'macro', 'weighted']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    recall_score(y_test,y_pred)\n",
    "except:\n",
    "    print('''ValueError: Target is multiclass but average='binary'. Please choose another average setting, \n",
    "          one of [None, 'micro', 'macro', 'weighted']''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9672820512820512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Parameters of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr=LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (penalty='l2',\n",
    "    dual=False,\n",
    "    tol=0.0001,\n",
    "    C=1.0,\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1,\n",
    "    class_weight=None,\n",
    "    random_state=None,\n",
    "    solver='lbfgs',\n",
    "    max_iter=100,\n",
    "    multi_class='auto',\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    n_jobs=None,\n",
    "    l1_ratio=None)\n",
    "\n",
    "    Docstring:     \n",
    "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "    scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "    cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "    (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "    'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "    This class implements regularized logistic regression using the\n",
    "    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "    that regularization is applied by default**. It can handle both dense\n",
    "    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "    floats for optimal performance; any other input format will be converted\n",
    "    (and copied).\n",
    "\n",
    "    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "    with primal formulation, or no regularization. The 'liblinear' solver\n",
    "    supports both L1 and L2 regularization, with a dual formulation only for\n",
    "    the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "    'saga' solver.\n",
    "\n",
    "    Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "    penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
    "        Used to specify the norm used in the penalization. The 'newton-cg',\n",
    "        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
    "        only supported by the 'saga' solver. If 'none' (not supported by the\n",
    "        liblinear solver), no regularization is applied.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    dual : bool, default=False\n",
    "        Dual or primal formulation. Dual formulation is only implemented for\n",
    "        l2 penalty with liblinear solver. Prefer dual=False when\n",
    "        n_samples > n_features.\n",
    "\n",
    "    tol : float, default=1e-4\n",
    "        Tolerance for stopping criteria.\n",
    "\n",
    "    C : float, default=1.0\n",
    "        Inverse of regularization strength; must be a positive float.\n",
    "        Like in support vector machines, smaller values specify stronger\n",
    "        regularization.\n",
    "\n",
    "    fit_intercept : bool, default=True\n",
    "        Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "        added to the decision function.\n",
    "\n",
    "    intercept_scaling : float, default=1\n",
    "        Useful only when the solver 'liblinear' is used\n",
    "        and self.fit_intercept is set to True. In this case, x becomes\n",
    "        [x, self.intercept_scaling],\n",
    "        i.e. a \"synthetic\" feature with constant value equal to\n",
    "        intercept_scaling is appended to the instance vector.\n",
    "        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "    class_weight : dict or 'balanced', default=None\n",
    "        Weights associated with classes in the form ``{class_label: weight}``.\n",
    "        If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "    random_state : int, RandomState instance, default=None\n",
    "        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "        data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
    "    - 'liblinear' and 'saga' also handle L1 penalty\n",
    "    - 'saga' also supports 'elasticnet' penalty\n",
    "    - 'liblinear' does not support setting ``penalty='none'``\n",
    "\n",
    "    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
    "    features with approximately the same scale. You can\n",
    "    preprocess the data with a scaler from sklearn.preprocessing.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "        If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "        label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "        across the entire probability distribution, *even when the data is\n",
    "        binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "        and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "    verbose : int, default=0\n",
    "        For the liblinear and lbfgs solvers set verbose to any positive\n",
    "        number for verbosity.\n",
    "\n",
    "    warm_start : bool, default=False\n",
    "        When set to True, reuse the solution of the previous call to fit as\n",
    "        initialization, otherwise, just erase the previous solution.\n",
    "        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "    n_jobs : int, default=None\n",
    "        Number of CPU cores used when parallelizing over classes if\n",
    "        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "        set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "        context. ``-1`` means using all processors.\n",
    "        See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "    l1_ratio : float, default=None\n",
    "        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "        combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "    classes_ : ndarray of shape (n_classes, )\n",
    "        A list of class labels known to the classifier.\n",
    "\n",
    "    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "        Coefficient of the features in the decision function.\n",
    "\n",
    "        `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "    intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "        Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "        If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "        `intercept_` is of shape (1,) when the given problem is binary.\n",
    "        In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "        outcome 0 (False).\n",
    "\n",
    "    n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "        Actual number of iterations for all classes. If binary or multinomial,\n",
    "        it returns only 1 element. For liblinear solver, only the maximum\n",
    "        number of iteration across all classes is given.\n",
    "\n",
    "        .. versionchanged:: 0.20\n",
    "\n",
    "            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
