{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20b43c3",
   "metadata": {},
   "source": [
    "### Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f961118",
   "metadata": {},
   "source": [
    "Ridge regression: it is one of regularization technique for linear regression problems\n",
    "\n",
    "\n",
    "•  maintains accuracy as well as a generalization of the mode\n",
    "•  reduces the magnitude of the variables, hence maintain all variables or features\n",
    "•  In simple words, \"In regularization technique, we reduce the magnitude of the features by keeping the same number of features\n",
    "\n",
    "How does Regularization Work?\n",
    "• by adding a penalty or complexity term to the complex model\n",
    "\n",
    "so in this:\n",
    "- a small amount of bias is added\n",
    "- reduces the complexity of the model, also called L2 regularization\n",
    "- cost function is altered by adding the penalty term to it\n",
    "- amount of bias added to the model is called Ridge Regression penalty\n",
    "- equation for the cost function in ridge regression will be img...\n",
    "\n",
    "\n",
    "From the cost function of Ridge Regression we can see that if the values of λ tends to zero, the equation becomes the cost function of the linear regression model..\n",
    "\n",
    "A general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used."
   ]
  },
  {
   "cell_type": "raw",
   "id": "28fe339d",
   "metadata": {},
   "source": [
    "-Tikhonov regularization\n",
    "-regularization technique that performs L2 regularization\n",
    "-modifies the loss function by adding the penalty (shrinkage quantity) that is equivalent to the square of the magnitude of coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d056d",
   "metadata": {},
   "source": [
    "$$\\displaystyle\\sum\\limits_{j=1}^m\\left(Y_{i}-W_{0}-\\displaystyle\\sum\\limits_{i=1}^nW_{i}X_{ji} \\right)^{2}+\\alpha\\displaystyle\\sum\\limits_{i=1}^nW_i^2=loss_{-}function+\\alpha\\displaystyle\\sum\\limits_{i=1}^nW_i^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cbf967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29d781",
   "metadata": {},
   "source": [
    "**alpha** − {float, array-like}, shape(n_targets)\n",
    "Alpha is the tuning parameter that decides how much we want to penalize the model.\n",
    "\n",
    "**fit_intercept** − Boolean\n",
    "Specifies that a constant (bias or intercept) should be added to the decision function or not\n",
    "\n",
    "**tol** − float, optional, default=1e-4\n",
    "It represents the precision of the solution.\n",
    "\n",
    "**normalize** − Boolean, optional, default = False\n",
    "If True, the regressor X will be normalized before regression. The normalization is done by subtracting the mean and dividing it by L2 norm. If fit_intercept = False, this parameter will be ignored.\n",
    "\n",
    "**copy_X** − Boolean, optional, default = True\n",
    "By default, it is true which means X will be copied. But if it is set to false, X may be overwritten.\n",
    "\n",
    "**max_iter** − int, optional\n",
    "As name suggest, it represents the maximum number of iterations taken for conjugate gradient solvers.\n",
    "\n",
    "**solver** − str, {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}’\n",
    "This parameter represents which solver to use in the computational routines. Following are the properties of options under this parameter\n",
    "\n",
    "    auto − It let choose the solver automatically based on the type of data.\n",
    "    svd − In order to calculate the Ridge coefficients, this parameter uses a Singular Value Decomposition of X.\n",
    "    cholesky − This parameter uses the standard scipy.linalg.solve() function to get a closed-form solution.\n",
    "    lsqr − It is the fastest and uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr.\n",
    "    sag − It uses iterative process and a Stochastic Average Gradient descent.\n",
    "    saga − It also uses iterative process and an improved Stochastic Average Gradient descent.\n",
    "    \n",
    "**random_state** − int, RandomState instance or None, optional, default = none\n",
    "- represents the seed of the pseudo random number generated which is used while shuffling the data. Following are the options −\n",
    "- if int, random_state is the seed used by random number generator.\n",
    "- if RandomState instance,random_state is the random number generator.\n",
    "- if None, the random number generator is the RandonState instance used by np.random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bea2b9",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8bb0d2",
   "metadata": {},
   "source": [
    "coef_ − array, shape(n_features,) or (n_target, n_features)\n",
    "\n",
    "This attribute provides the weight vectors.\n",
    "\n",
    "Intercept_ − float | array, shape = (n_targets)\n",
    "It represents the independent term in decision function.\n",
    "\n",
    "n_iter_ − array or None, shape (n_targets)\n",
    "Available for only ‘sag’ and ‘lsqr’ solver, returns the actual number of iterations for each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f018e46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7629498741931634"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "n_samples, n_features = 15, 10\n",
    "rs = np.random.RandomState(0)\n",
    "y = rs.randn(n_samples)\n",
    "X = rs.randn(n_samples, n_features)\n",
    "model = Ridge(alpha = 0.5)\n",
    "model.fit(X, y)\n",
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa696d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32720254, -0.34503436, -0.2913278 ,  0.2693125 , -0.22832508,\n",
       "       -0.8635094 , -0.17079403, -0.36288055, -0.17241081, -0.43136046])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cebf58c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5274865723969377"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae413d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
