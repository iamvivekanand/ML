{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74d9fa8",
   "metadata": {},
   "source": [
    "### SVM- A guide to beginners AnalyticsVidya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a74ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is a SVM?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a12c6a73",
   "metadata": {},
   "source": [
    "its a supervised machine learning algorithm that tries to find the best hyper to separate two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e112a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the diffrence between SVM and Logistic Regression??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b7e353c",
   "metadata": {},
   "source": [
    "Both the algorithms try to find the best hyperplane, but the main difference is logistic regression is a probabilistic approach whereas support vector machine is based on statistical approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6382addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When to use logistic regression vs Support vector machine?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bebe0414",
   "metadata": {},
   "source": [
    "SVM works best when the dataset is small and complex. It is usually advisable to first use logistic regression and see how does it performs, if it fails to give a good accuracy you can go for SVM without any kernel\n",
    "\n",
    "Logistic regression and SVM without any kernel have similar performance but depending on your features, one may be more efficient than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f266f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of Support Vector Machine"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72330d1c",
   "metadata": {},
   "source": [
    "Linear SVM:\n",
    "When the data is perfectly linearly separable only then we can use Linear SVM. Perfectly linearly separable means that the data points can be classified into 2 classes by using a single straight line(if 2D)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6f71fd9",
   "metadata": {},
   "source": [
    "Non-Linear SVM\n",
    "When the data is not linearly separable then we can use Non-Linear SVM, which means when the data points cannot be separated into 2 classes by using a straight line (if 2D) then we use some advanced techniques like kernel tricks to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e884451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terminologies in SVM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "280d550d",
   "metadata": {},
   "source": [
    "WHAT ARE SUPPORT VECTORS??\n",
    "These are the points that are closest to the hyperplane. \n",
    "A separating line will be defined with the help of these data points."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b67f87b4",
   "metadata": {},
   "source": [
    "WHAT IS MARGIN? DEFINE HARD MARGIN AND SOFT MARGIN?\n",
    "\n",
    "Margin is the distance between the hyperplane and support vectors. In SVM large margin is considered a good margin. There are two types of margins hard margin and soft margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa21cc",
   "metadata": {},
   "source": [
    "<img src='svm11.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891de41",
   "metadata": {},
   "source": [
    "<img src='svm11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae3fa1",
   "metadata": {},
   "source": [
    "### How does Support Vector Machine work?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab3287a8",
   "metadata": {},
   "source": [
    "it tries to find the best hyperplane which can separate both classes with maximum margin form nearrest point of both the classes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef8da1b4",
   "metadata": {},
   "source": [
    "-it tries to find the best hyperplane recursively untill it has maximum margin from both the classes.\n",
    "-there can be drwan many hyperplane through two classes in order to separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01268cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why those points are called support vectors??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01f26ec2",
   "metadata": {},
   "source": [
    "Those points are called support vectors because they are contributing towards development of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "899e938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a data point is not an SV, removing it has no effect on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b996ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What will happen if we delete the support vector?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5214431",
   "metadata": {},
   "source": [
    "Deleting the SV will then change the position of the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b10f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we define the dimension of the hyperplane??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae9cc80d",
   "metadata": {},
   "source": [
    "The dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be0e2b",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (SVC):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5780cbe",
   "metadata": {},
   "source": [
    "Many have confusion with the terms SVM and SVC, the simple answer is if the hyperplane that we are using for classification is in linear condition, then the condition is SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f69cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC,SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b34febf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is soft margin in Support Vector Machine Algorithm? Define Margin Violation in SVM?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f3e366c",
   "metadata": {},
   "source": [
    "As most of the real-world data are not fully linearly separable, we will allow some margin violation to occur which is called soft margin classification.\n",
    "\n",
    "It is better to have a large margin, even though some constraints are violated. \n",
    "\n",
    "Margin violation means choosing a hyperplane. Which can allow some data points to stay on either the incorrect side of the hyperplane and between the margin and correct side of the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ed49f",
   "metadata": {},
   "source": [
    "### Basic Parameters for SVM – (Dramatic/Side portion) (both linear and non-linear SVMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ed7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=SVC(C=1.0,\n",
    "        kernel='rbf',\n",
    "        degree=3,\n",
    "        gamma='scale',\n",
    "        coef0=0.0,\n",
    "        shrinking=True,\n",
    "        probability=False,\n",
    "        tol=0.001,\n",
    "        cache_size=200,\n",
    "        class_weight=None,\n",
    "        verbose=False,\n",
    "        max_iter=-1,\n",
    "        decision_function_shape='ovr',\n",
    "        break_ties=False,\n",
    "        random_state=None,)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb516160",
   "metadata": {},
   "source": [
    "Regularization parameter (C): \n",
    "- Penalty parameter of the error term.\n",
    "- or degree of optimization \n",
    "-in a simple word, it suggests the model choose data points as a support vector\n",
    "-For large C – then model choose more data points as a support vector and we get the higher variance and lower bias, which may lead to the problem of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8174826",
   "metadata": {},
   "source": [
    "<img src='svm22.jpg'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2256a7fc",
   "metadata": {},
   "source": [
    "For small C – If the value of C is small then the model chooses fewer data points as a support vector and gets lower variance/high bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ceeb9",
   "metadata": {},
   "source": [
    "<img src='svm23.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c7ac916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma Parameter:  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a91b68a0",
   "metadata": {},
   "source": [
    "-used with Gaussian RBF kernel.\n",
    "-If we are useing linear or polynomial kernel then we do not need GAMMA parameter, only C hypermeter is used.\n",
    "-It decides that how much curvature we want in a decision boundary.\n",
    "-High Gamma value – More curvature, Low Gamma value – Less curvature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ace41",
   "metadata": {},
   "source": [
    "<img src='svm24.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43632561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c955694d",
   "metadata": {},
   "source": [
    "We choose svm kernel based on our dataset type, if it has linear pattern w.r.t target then we use linear kernel else we use.. non linear kernel.."
   ]
  },
  {
   "cell_type": "raw",
   "id": "672e1bdd",
   "metadata": {},
   "source": [
    "linear kernel='linear'\n",
    "non-linear kernel= ‘rbf’, ‘poly’ and ‘sigmoid’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82086c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "269ff639",
   "metadata": {},
   "source": [
    "1. It controls the flexibility of the decision boundary.\n",
    "2. Higher degrees yield more flexible decision boundaries.\n",
    "3. Highly recommended for polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4edf9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random State:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbbaf842",
   "metadata": {},
   "source": [
    "This particular parameter, 50-50 people use it and others won’t. This is not so important, ensures that the splits that you generate are reproducible, like we give mostly the value as 0 or 1 or it may be any other number too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1880ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
