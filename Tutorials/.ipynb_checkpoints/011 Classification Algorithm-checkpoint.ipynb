{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification is the problem of identifying the class of a new observation out of available class inferred from training data\n",
    "- Classification algorithms deals with prediction of discreate or categorical target\n",
    "- SML algorithm can be broadly classified into Regression and Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is of two types:  \n",
    "\n",
    "    1.Binary Classification:\n",
    "    When there is 2 distinct classes, like Yes|NO, True|False\n",
    "    \n",
    "    2.Multiclass Classification: The number of classes is more than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Information Gain and how it is calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Information gain is the reduction in entropy or surprise by transforming a dataset and is often used in training decision trees. \n",
    "- Information gain is calculated by comparing the entropy of the dataset before and after a transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are Learners in Classification Problems??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Learners are algorirthm used to train the model, they learn from data\n",
    "\n",
    "    There are two types of learners in classification\n",
    "    1.Lazy Learner\n",
    "    2.Eager Learner\n",
    "\n",
    "    -lazy lerner takes more time in prediction\n",
    "    -while an eager learner takes less time in prediction,\n",
    "    -LAZY Learner and EAGER Learner are classified on the basis of time taken while doing prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe in detail about Lazy Learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy Learners:\n",
    "- stores the training dataset and wait until it receives the test dataset\n",
    "- classification is done on the basis of the most related data stored in the training dataset.\n",
    "- less time in training \n",
    "- more time for predictions\n",
    "- K-NN algorithm, Case-based reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe in detail about Eager Learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager Learners:\n",
    "- develop a classification model using training dataset before receiving a test dataset\n",
    "- more time in training\n",
    "- less time in prediction\n",
    "- e.g.  Decision Trees, Naïve Bayes, ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of ML Classification Algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Classification Algorithms can be further divided into the Mainly two category:\n",
    "    1.Linear Models\n",
    "        Logistic Regression\n",
    "        Support Vector Machines\n",
    "    2.Non-linear Models\n",
    "        K-Nearest Neighbours\n",
    "        Kernel SVM\n",
    "        Naïve Bayes\n",
    "        Decision Tree Classification\n",
    "        Random Forest Classification\n",
    "        etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some evaluation metrics for regression??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean square error\n",
    "- Root mean square error\n",
    "- f1 Score\n",
    "- R2 \n",
    "- Adjusted R2\n",
    "- Mean absoulte error\n",
    "- Mean absolute percentage error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error:\n",
    "- Average of sum of the absolute difference between the predicted and actual values\n",
    "- MAE does not indicate the direction of the model i.e. no indication about underperformance or overperformance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mae.jpg' height=200 width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from sklearn.metrics import mean_absolute_error as mae ,mean_absolute_percentage_error as mape\n",
    "    print(mae(y_test,y_pred))\n",
    "    print(mape(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEAN SQAURED ERROR:\n",
    "- average of sum of squared difference between actual and predicted value\n",
    "- MSE is like the MAE, but the only difference is that the it squares the difference of actual and predicted output values before summing them all instead of using the absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mse.jpg' height=150 width=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROOT MEAN SQAURED ERROR:\n",
    "- rmse=(mean_squared_error(y_test,y_pred))**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics for a Classification model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some famous metrics for classification model.\n",
    "\n",
    "    1. confusion matrix\n",
    "    2. accuracy score\n",
    "    3. Classication Report (Precision,recall,f1-score,support)\n",
    "    4. specificity\n",
    "    5. roc_auc_score\n",
    "    6. roc_curve\n",
    "    7. auc (area under roc curve)\n",
    "    8. f1_score \n",
    "    9. log_los (logarithmic loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of Number of correct predictions made to the all predictions made\n",
    "\n",
    "    Accuracy   =   TP+TN / TP+TN+FP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This report consists of the scores of Precisions, Recall, F1 and Support\n",
    "        \n",
    "    Precision: TP/TP+FP\n",
    "    Mostly used in documents retrieval, defined as no of correct documents retrieved by the model to the total correct document in the queried data\n",
    "    \n",
    "    \n",
    "    Recall or Sensitivity: TP/TP+FN\n",
    "    \n",
    "    Specificity:TN/TN+FP\n",
    "    Specificity, in contrast to recall, may be defined as the number of negatives returned by our ML model\n",
    "\n",
    "    Support: Number of samples of the True Response that lies in each class of target values.\n",
    "    \n",
    "    F1 Score:\n",
    "    -This score will give us the harmonic mean of precision and recall. \n",
    "    -Mathematically, F1 score is the weighted average of the precision and recall. \n",
    "    -The best value of F1 would be 1 and worst would be 0\n",
    "\n",
    "    𝑭𝟏 = 𝟐 ∗ (𝒑𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏 ∗ 𝒓𝒆𝒄𝒂𝒍𝒍) / (𝒑𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏 + 𝒓𝒆𝒄𝒂𝒍𝒍)\n",
    "\n",
    "    from sklearn.metrics import 𝑭𝟏_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Log Loss/logarithmic loss/Cross-Entropy Loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- also called Logistic regression loss or cross-entropy loss.\n",
    "- Used for a classifier whose output is a probability value between the 0 and 1.\n",
    "- Lower log loss higher accuracy\n",
    "- For a good binary Classification model, the value of log loss should be near to 0.\n",
    "\n",
    "As we know that accuracy is the count of predictions (predicted value = actual value) in our model whereas Log Loss is the amount of uncertainty of our prediction based on how much it varies from the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets us see how we can calculate all these score\n",
    "\n",
    "from sklearn.metrics import log_loss,accuracy_score,f1_score,auc,roc_auc_score,roc_curve,precision_score,recall_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "y_pred = [1, 0, 1, 1, 1, 0, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.815750437193334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60         6\n",
      "           1       0.50      0.75      0.60         4\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.62      0.62      0.60        10\n",
      "weighted avg       0.65      0.60      0.60        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Or simply we can print all these values using classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Confusion Matrix:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-Known as the error matrix.\n",
    "-Consists predictions in summarized form, it has a total number of correct predictions and incorrect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='confusion-matrix.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. AUC (area under the roc_curve):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ROC: Receiver Operating Characteristics Curve \n",
    "- AUC: Area Under the Curve.\n",
    "- we use the AUC-ROC Curve for visualizing the performance of the multi-class classification model\n",
    "- it shows the performance of the classification model at different thresholds\n",
    "\n",
    "  The ROC curve is plotted with TPR (True Positive Rate) on Y-axis and FPR(False Positive Rate) on X-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc_auc_score(y_test,y_pred)\n",
    "\n",
    "roc_curve(y_test,y_score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
